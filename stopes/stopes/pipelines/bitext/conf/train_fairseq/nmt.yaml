# @package train_fairseq

_target_: stopes.modules.train_fairseq_module.TrainFairseqModule

defaults:
  - params/model: transformer

output_dir: ???
# number of gpus to use for training the model
num_gpus: 1
# number of gpus per node in your cluster
num_gpus_per_node: 8
params:
  # Default fairseq config.
  # We override all absolute interpolation.
  common:
    log_interval: 500
    log_format: simple
    no_progress_bar: true
    model_parallel_size: 1
    fp16: false
    memory_efficient_fp16: false
    tpu: false
  checkpoint:
    save_interval: 1
    save_dir: ${...output_dir}
    model_parallel_size: ${..common.model_parallel_size}
  dataset:
    batch_size: null
    batch_size_valid: ${.batch_size}
    max_tokens: 2000
    max_tokens_valid: ${.max_tokens}
    grouped_shuffling: false
    update_epoch_batch_itr: ${.grouped_shuffling}
  task:
    _name: translation
    source_lang: ???
    target_lang: ???
    data: ???
  optimization:
    max_epoch: 100
    lr: [1e-3]
    stop_min_lr: 1e-9
    clip_norm: 0.0
    update_freq: [4]
  criterion:
    _name: label_smoothed_cross_entropy
    label_smoothing: 0.2
  optimizer:
    _name: adam
    adam_betas: (0.9, 0.98)
    weight_decay: 0.0001
  lr_scheduler:
    _name: inverse_sqrt
    warmup_updates: 4000
    warmup_init_lr: 1e-7
  bpe: none
  scoring: none
  distributed_training:
    distributed_world_size: ${...num_gpus}
    fp16: ${..common.fp16}
    memory_efficient_fp16: ${..common.memory_efficient_fp16}
    tpu: ${..common.tpu}
  bmuf:
    distributed_world_size: ${..distributed_training.distributed_world_size}
